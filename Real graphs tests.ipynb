{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7054c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "import numpy as np\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "import sys\n",
    "sys.path += ['/home/lorenzo/Scrivania/My_projects/DeepWalk/P2Vec/Package/']  \n",
    "\n",
    "from node2vec.model import Node2Vec\n",
    "\n",
    "from p2vec import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "directory = '/home/lorenzo/Scrivania/My_projects/DeepWalk/P2Vec/dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d8558a",
   "metadata": {},
   "source": [
    "# Qui direi abbastanza bene. Bisogna solo fare un po' attenzione al parametro $\\gamma$. `orkut` ci mette molto tempo e per `Node2Vec` non l'ho nemmeno testato."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c618c33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 26931\n",
      "Number of clusters: 608\n"
     ]
    }
   ],
   "source": [
    "# The datasets are ['amazon', 'dblp', 'livejournal', 'orkut', 'youtube']\n",
    "\n",
    "name = 'youtube'\n",
    "\n",
    "EL = pd.read_csv(directory + name + '.csv')[['id1', 'id2']]\n",
    "n = len(pd.concat([EL.id1, EL.id2]).unique())\n",
    "\n",
    "ℓtrue = pd.read_csv(directory + name + '_label.csv').set_index('node')\n",
    "ℓtrue = ℓtrue.loc[np.arange(n)].label.values\n",
    "n_clusters = len(np.unique(ℓtrue))\n",
    "\n",
    "\n",
    "print(\"Number of nodes: \" + str(n))\n",
    "print(\"Number of clusters: \" + str(n_clusters))\n",
    "\n",
    "\n",
    "A = csr_matrix((np.ones(len(EL)), (EL.id1, EL.id2)), shape = (n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4130eb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the optimization for k = 1\n",
      "[========================>] 100%, η = 0.001968\n",
      "\n",
      "Computing the parameters values...\n",
      "\n",
      "Execution time: 18.45024037361145\n",
      "\n",
      "\n",
      "\n",
      "Clustering 26931 points in 128D to 608 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 24 (8.01 s, search 7.73 s): objective=2097.28 imbalance=2.725 nsplit=0       \n",
      "------------------------------\n",
      "\n",
      "The NMI is: 0.6456827401351222\n"
     ]
    }
   ],
   "source": [
    "# Our method\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "d = A@np.ones(n)\n",
    "D_1 = diags(d**(-1))\n",
    "P = D_1.dot(A)\n",
    "\n",
    "Pv = [P]\n",
    "dim = 128\n",
    "k = 1\n",
    "walk_length = 5\n",
    "γ = 0.3\n",
    "\n",
    "p2v = CreateEmbedding(Pv, dim = dim, walk_length = walk_length, k = k, γ = γ) \n",
    "print(\"\\nExecution time: \" + str(time.time() - t0) + \"\\n\\n\")\n",
    " \n",
    "# run Kmeans\n",
    "Φ = p2v.Φ\n",
    "n_clusters = n_clusters\n",
    "\n",
    "kmeans = faiss.Kmeans(Φ.shape[1], n_clusters, verbose = True)\n",
    "kmeans.train(np.ascontiguousarray(Φ).astype('float32'))\n",
    "_, ℓ = kmeans.assign(np.ascontiguousarray(Φ).astype('float32'))\n",
    "\n",
    "print(\"\\n------------------------------\")\n",
    "print(\"\\nThe NMI is: \" + str(normalized_mutual_info_score(ℓ, ℓtrue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82681805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing progress: 0%Learning embeddings using the Skip-Gram model...\n",
      "Preprocessing progress: 99.14%95.8%ng progress: 98.498.4%%%%\n",
      "Preprocessing transition probabilities: done!\n",
      "Walking progress: 96.54%g progress: 0%0%0%g progress: \n",
      "Simulating random walks: done!\n",
      "Learning embeddings: done!\n",
      "\n",
      "\n",
      "Execution time: 59.04410696029663\n",
      "\n",
      "Clustering 26931 points in 128D to 608 clusters, redo 1 times, 25 iterations\n",
      "  Preprocessing in 0.00 s\n",
      "  Iteration 24 (8.28 s, search 8.01 s): objective=82601.3 imbalance=2.023 nsplit=0       \n",
      "--------------------------------\n",
      "\n",
      "The NMI is: 0.6446431446311961\n"
     ]
    }
   ],
   "source": [
    "# Node2Vec\n",
    "\n",
    "t0 = time.time()\n",
    "node2vec_model = Node2Vec(EL.id1, EL.id2, graph_is_directed = False)\n",
    "node2vec_model.simulate_walks()\n",
    "node2vec_model.learn_embeddings(dimensions = dim, context_size = walk_length)\n",
    "Y = node2vec_model.embeddings\n",
    "print(\"\\n\\nExecution time: \" + str(time.time() - t0) + \"\\n\")\n",
    "\n",
    "# run kmeans\n",
    "kmeans = faiss.Kmeans(Y.shape[1], n_clusters, verbose = True)\n",
    "kmeans.train(np.ascontiguousarray(Y).astype('float32'))\n",
    "_, ℓDW = kmeans.assign(np.ascontiguousarray(Y).astype('float32'))\n",
    "\n",
    "print(\"\\n--------------------------------\")\n",
    "print(\"\\nThe NMI is: \" + str(normalized_mutual_info_score(ℓDW, ℓtrue)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc5cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
