{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7054c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix#, diags\n",
    "import numpy as np\n",
    "# from sklearn.metrics.cluster import normalized_mutual_info_score as nmi\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# from os import listdir\n",
    "\n",
    "import sys\n",
    "sys.path += ['../Package/']  \n",
    "\n",
    "# node2vec package can be installed following instructions at https://github.com/thibaudmartinez/node2vec\n",
    "from node2vec.model import Node2Vec\n",
    "\n",
    "from node_embedding import *\n",
    "from dcsbm import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "directory = '../dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333597eb",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "In this notebook we show how to reproduce the results of community detection obtained in Table 1 of *Efficient distributed representations beyond negative sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05bc172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the optimization for k = 1\n",
      "0.9239899207534861 7.692234754562378\n",
      "\n",
      "...............................\n",
      "\n",
      "Preprocessing progress: 0%0%Learning embeddings using the Skip-Gram model...\n",
      "Preprocessing progress: Preprocessing progress: 92.3%92.3%2%\n",
      "Preprocessing transition probabilities: done!\n",
      "Walking progress: 96.6%%4.4%ogress: Walking progress: Walking progress: Walking progress: \n",
      "Simulating random walks: done!\n",
      "Learning embeddings: done!\n",
      "0.9275918052591813 19.68018865585327\n"
     ]
    }
   ],
   "source": [
    "name = 'amazon' # select the dataset\n",
    "# available names are :'amazon', 'dblp', 'livejournal', 'youtube'\n",
    "\n",
    "# load the dataset\n",
    "EL = pd.read_csv(directory + name + '.csv')[['id1', 'id2']]\n",
    "n = len(pd.concat([EL.id1, EL.id2]).unique())\n",
    "\n",
    "# load the label vector\n",
    "ℓtrue = pd.read_csv(directory + name + '_label.csv').set_index('node')\n",
    "ℓtrue = ℓtrue.loc[np.arange(n)].label.values\n",
    "n_clusters = len(np.unique(ℓtrue))\n",
    "\n",
    "# create the adjacency matrix and the degree vector\n",
    "A = csr_matrix((np.ones(len(EL)), (EL.id1, EL.id2)), shape = (n,n))    \n",
    "d = A@np.ones(A.shape[0])\n",
    "\n",
    "##############################################################\n",
    "# Generate the embeddings\n",
    "\n",
    "dim = 128\n",
    "\n",
    "# EDRep\n",
    "t0 = time.time()\n",
    "f_func = lambda x:np.sqrt(x)\n",
    "Φ = NodeEmbedding(A, dim, f_func = f_func, η = .85, n_epochs = 30, n_prod = 1, \n",
    "                verbose = True, cov_type = 'full')\n",
    "print(computeScore(Φ, ℓtrue), time.time() - t0)\n",
    "\n",
    "print('\\n...............................\\n')\n",
    "# Negative sampling\n",
    "t0 = time.time()\n",
    "X = Node2VecNS(A, dim, verbose = True)\n",
    "print(computeScore(X, ℓtrue), time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc31afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
